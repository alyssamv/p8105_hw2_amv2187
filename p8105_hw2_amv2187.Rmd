---
title: "p8105_hw2_amv2187"
author: "Alyssa Vanderbeek"
date: "5 October 2018"
output: github_document
---

## Problem 1


```{r}
knitr::opts_chunk$set(echo = TRUE)

library(readxl)
library(p8105.datasets)
library(tidyverse)

getwd()
```

```{r data_clean, warning=FALSE, }
transit_data_clean = read_csv("data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv", col_types = cols()) %>%
  janitor::clean_names() %>%
  select(line, station_name, station_latitude, station_longitude, route1:route11, entry, vending, entrance_type, ada) %>%
  mutate(entry = recode(entry, 'YES' = TRUE, 'NO' = FALSE))

str(transit_data_clean) # display head of transit dataset
```

The cleaned dataset contains information about subway stations: the station name, the line it runs, its latitude and longitudinal location, the routes it services, whether entry exists and of what kind (stairs vs. elevator), and whether the station is ADA compliant. To learn and clean the data, I first imported and standardized the column names. I looked at the list of columns to understand what variables are available. Then I examined the unique values in a few of the columns to get a sense of how many distinct values there were. Lastly, I checked whether or not there were any missing values in the "Route 1" column. This would determine whether there were any stations (row entries) that had no routes associated. This would have been redundant and I would filter these rows out of the dataset. None were found. After cleaning the data as instructed, the dataset dimension are `r nrow(transit_data_clean)` rows by `r ncol(transit_data_clean)` columns. As of now, this dataset is not tidy.

```{r}
# number of distinct stations, defined by both station name and line
n_distinct_stations = transit_data_clean %>%
  distinct(station_name, line) %>% # get distinct combinations for station name and line
  nrow # count number of distinct stations

# number of ADA compliant stations
n_ada_compliant_stations = transit_data_clean %>%
  distinct(station_name, line, ada) %>% # get distinct stations and their ADA compliance
  filter(ada == TRUE) %>% # filter only those stations that are ADA compliant
  nrow # count

# proportion of stations entrances/exits without vending allow entrance
prop_no_vending_yes_entrance = transit_data_clean %>%
  filter(vending == 'NO') %>% # filter to station entrances/exits that don't have vending
  summarise(sum(entry) / n()) # get proportion of access points with entry
  
```

There are `r n_distinct_stations` distinct stations, of which `r n_ada_compliant_stations` are ADA compliant. Of the station access points that do not have vending, `r round(prop_no_vending_yes_entrance * 100, 0)`% allow entrance.

```{r}
# create tidy dataset
transit_data_tidy = transit_data_clean %>%
  gather(key = 'route', value = 'train', route1:route11) %>% # reformat to long, with single column for route
  mutate(route = substr(route, 6, nchar(route))) %>% # change values in route designation column. e.g. 'route1' becomes '1'
  distinct %>% # get distinct rows
  filter(!is.na(train)) # filter out rows that don't have a route designation

str(transit_data_tidy)

# number of stations that service the A train
A_train_station = transit_data_tidy %>%
  filter(train == 'A') %>% # filter to only stations that service A
  distinct %>%
  nrow # count

# number of stations that serive the A train and are ADA compliant
ADA_compliant_A_train = transit_data_tidy %>%
  filter(train == 'A', ada == TRUE) %>% # filter to only stations that service A and are ADA compliant
  distinct %>%  
  nrow # count
```

Additionally, `r A_train_station` stations service the A train, of which `r ADA_compliant_A_train` are ADA compliant.


## Problem 2

```{r}
## Mr. Trash Wheel data
trash_wheel_data = read_excel("data/HealthyHarborWaterWheelTotals2018-7-28.xlsx", 
    sheet = "Mr. Trash Wheel", range = cell_cols(1:14)) %>%
  janitor::clean_names() %>%
  filter(!is.na(dumpster) & month %in% month.name) %>%  # only dumpster-specific rows; i.e. dumpster number specified and row values are not totals
  mutate(sports_balls = as.integer(round(sports_balls))) # round number of sports balls to nearest integer

str(trash_wheel_data) # display column names and type

## Precipitation data
# 2016
precipitation_2016 = readxl::read_excel("data/HealthyHarborWaterWheelTotals2018-7-28.xlsx", 
    sheet = "2016 Precipitation", skip = 1) %>% # select sheet and assign first row as column names
  janitor::clean_names() %>%
  filter(!is.na(month)) %>% # filter rows that do not correspond to a month
  mutate(year = '2016') # assign year designation

str(precipitation_2016)

# 2017
precipitation_2017 = readxl::read_excel("data/HealthyHarborWaterWheelTotals2018-7-28.xlsx", 
    sheet = "2017 Precipitation", skip = 1) %>%
  janitor::clean_names() %>%
  filter(!is.na(month) & !is.na(total)) %>%
  mutate(year = '2017')

str(precipitation_2017)

# Combined both years
precipitation_combined = bind_rows(precipitation_2016, precipitation_2017) %>%
  mutate(month = month.name[month]) # create month as character with labels January through December

str(precipitation_combined)
```

The trash wheel dataset contains information about dumpsters that are sent to a waste-to-energy facility once they become full. Over the course of about `r round(difftime(max(trash_wheel_data$date), min(trash_wheel_data$date), units = 'days')/365)` years, `r nrow(trash_wheel_data)` were filled and transported to the facility. For each dumpster, information was collected regarding its weight, volume, and the type of waste it contained. This includes the number of glass bottles, plastic bags, and sports balls, to name a few. For example, from this data we know that the median number of sports balls collected in 2016 was `r trash_wheel_data %>% filter(year == '2016') %>% summarise(median_sports_balls = median(sports_balls))`. The number of homes that each dumpster powered is also estimated.  

Notably, dumpsters are filled more frequently in periods of precipitation. The precipitation dataset contains the total rainfall (in inches) for each month in the years 2016 and 2017. The average rainfall in 2016 was `r round(mean(precipitation_2016$total), 2)` inches, with a high of `r max(precipitation_2016$total)` inches in `r month.name[which(precipitation_2016$total == max(precipitation_2016$total))]`, and a low of `r min(precipitation_2016$total)` inches in `r month.name[which(precipitation_2016$total == min(precipitation_2016$total))]`. In 2017, the average rainfall was `r round(mean(precipitation_2017$total), 2)` inches, with a high of `r max(precipitation_2017$total)` inches in `r month.name[which(precipitation_2017$total == max(precipitation_2017$total))]`, and a low of `r min(precipitation_2017$total)` inches in `r month.name[which(precipitation_2017$total == min(precipitation_2017$total))]`. The total precipitation in 2017 was `r sum(precipitation_2017$total)` inches.


## Problem 3

```{r}
data("brfss_smart2010") 

brfss_cleaned = brfss_smart2010 %>%
  janitor::clean_names() %>%
  filter(topic == 'Overall Health') %>%
  select(-class, -topic, -question, -sample_size, -confidence_limit_low:-geo_location) %>% # select all columns except for the ones listed
  spread(key = response, value = data_value) %>% # reformat to wide by response selection
  janitor::clean_names() %>% # clean names again, since there are new columns
  mutate(excellent_or_very_good = excellent + very_good) # new column that tabulates the proportion of responses that were either 'Excellent' or 'Very Good'

str(brfss_cleaned)
```

```{r}
# number of unique locations in the data
num_unique_locations = brfss_cleaned %>%
  distinct(locationdesc) %>%
  nrow

# which states are not included in the dataset?
missing_states = which(!(state.abb %in% unique(brfss_cleaned$locationabbr)))

# are there any 'extra' states?
extra_locns = unique(brfss_cleaned$locationabbr)[which(!(unique(brfss_cleaned$locationabbr) %in% state.abb))]

# Function to get the mode(s) of a given variables (column)
Modes <- function(x) {
  t = which(table(x) == max(table(x)))
  return(paste(names(t), collapse = ','))
}

# identify the most commonly surveyed state 
most_common_state = Modes(brfss_cleaned$locationabbr)
```

`r brfss_cleaned %>% distinct(locationdesc) %>% nrow` distinct locations were surveyed in the BRFSS data, and every state including Washington, DC is represented. `r state.name[match(most_common_state, state.abb)]` was observed most frequently. 

In 2002, the median proportion of "Excellent" responses was `r brfss_cleaned %>% filter(year == 2002) %>% summarise(m = median(excellent, na.rm = T))`%.

```{r}
# Histogram of "Excellent" responses in 2002
brfss_cleaned %>%
  filter(year == 2002) %>% # only year 2002
  ggplot(., aes(x = excellent)) +
  geom_histogram() + # histogram
  labs(
    title = 'Nationwide "Excellent" responses (2002)',
    y = 'Number of counties',
    x = 'Proportion of "Excellent" responses'
  ) +
  theme_bw() # make plot background white

# Scatterplot of "Excellent" responses in New York and Queens counties 2002-2010
brfss_cleaned %>%
  filter(grepl('New York County|Queens County', locationdesc) == T) %>% # filter only to New York and Queens counties
  ggplot(., aes(x = year, y = excellent, color = locationdesc)) +
  geom_point() + # scatterplot
  geom_smooth(method = 'loess', se = F) + # add loess fit
  labs(
    title = 'Proportion of "Excellent" responses 2002-2010',
    y = 'Proportion of "Excellent" responses',
    x = 'Year'
  ) +
  viridis::scale_color_viridis( # change color scheme
    name = "Location",
    discrete = TRUE
  ) +
  theme_bw() + # make plot background white
  theme(legend.position = "bottom") # move legend to underneath the plot
```

